## Description

The model is trained using (https://github.com/lavis-nlp/spert).
To reproduce the results, please download the annotated data and train it with SpERT.

## Description
Let people know what your project can do specifically. Provide context and add a link to any reference visitors might be unfamiliar with. A list of Features or a Background subsection can also be added here. If there are alternatives to your project, this is a good place to list differentiating factors.

## Badges
On some READMEs, you may see small images that convey metadata, such as whether or not all the tests are passing for the project. You can use Shields to add some to your README. Many services also have instructions for adding a badge.

## Visuals
Depending on what you are making, it can be a good idea to include screenshots or even a video (you'll frequently see GIFs rather than actual videos). Tools like ttygif can help, but check out Asciinema for a more sophisticated method.

## Installation
Within a particular ecosystem, there may be a common way of installing things, such as using Yarn, NuGet, or Homebrew. However, consider the possibility that whoever is reading your README is a novice and would like more guidance. Listing specific steps helps remove ambiguity and gets people to using your project as quickly as possible. If it only runs in a specific context like a particular programming language version or operating system or has dependencies that have to be installed manually, also add a Requirements subsection.

## Usage
Use examples liberally, and show the expected output if you can. It's helpful to have inline the smallest example of usage that you can demonstrate, while providing links to more sophisticated examples if they are too long to reasonably include in the README.

## Support
Tell people where they can go to for help. It can be any combination of an issue tracker, a chat room, an email address, etc.

## Roadmap
If you have ideas for releases in the future, it is a good idea to list them in the README.

## References
<a id="1">[1]</a> 
A. Vanzo, D. Croce, E. Bastianelli, R. Basili, and D. Nardi,
“Grounded language interpretation of robotic commands through
structured learning,” Artificial Intelligence, vol. 278, 2020.
<a id="2">[2]</a> 
K. Dukes, “SemEval-2014 task 6: Supervised semantic parsing of
robotic spatial commands,” in Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014). Dublin, Ireland: Association for Computational Linguistics, Aug. 2014, pp. 45–53.
<a id="3">[3]</a> 
R. Scalise, S. Li, H. Admoni, S. Rosenthal, and S. S. Srinivasa, “Natural
language instructions for human–robot collaborative manipulation,”
The International Journal of Robotics Research, vol. 37, no. 6, pp.
558–565, 2018.
<a id="4">[4]</a> 
M. Eberts and A. Ulges, “Span-based joint entity and relation extraction
with transformer pre-training,” arXiv preprint arXiv:1909.07755,
2019.
## Authors and acknowledgment
Show your appreciation to those who have contributed to the project.

## License
For open source projects, say how it is licensed.

